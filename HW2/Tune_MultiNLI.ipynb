{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('hw2_data/snli_train.tsv',delimiter='\\t')\n",
    "val = pd.read_csv(\"hw2_data/snli_val.tsv\",delimiter='\\t')\n",
    "\n",
    "\n",
    "train_m = pd.read_csv('hw2_data/mnli_train.tsv',delimiter='\\t')\n",
    "val_m = pd.read_csv(\"hw2_data/mnli_val.tsv\",delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fiction', 'telephone', 'slate', 'government', 'travel'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_m['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[['sentence1','sentence2']]\n",
    "train_targets = np.array(train['label'])\n",
    "\n",
    "val_data = val[['sentence1','sentence2']]\n",
    "val_targets = np.array(val['label'])\n",
    "\n",
    "\n",
    "train_data_m = train_m[['sentence1','sentence2','genre']]\n",
    "train_targets_m = np.array(train_m['label'])\n",
    "\n",
    "val_data_m = val_m[['sentence1','sentence2','genre']]\n",
    "val_targets_m = np.array(val_m['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = np.where(train_targets=='entailment',1,train_targets)\n",
    "train_targets = np.where(train_targets=='neutral',0,train_targets)\n",
    "train_targets = np.where(train_targets=='contradiction',2,train_targets)\n",
    "\n",
    "\n",
    "val_targets = np.where(val_targets=='entailment',1,val_targets)\n",
    "val_targets = np.where(val_targets=='neutral',0,val_targets)\n",
    "val_targets = np.where(val_targets=='contradiction',2,val_targets)\n",
    "\n",
    "train_targets_m = np.where(train_targets_m=='entailment',1,train_targets_m)\n",
    "train_targets_m = np.where(train_targets_m=='neutral',0,train_targets_m)\n",
    "train_targets_m = np.where(train_targets_m=='contradiction',2,train_targets_m)\n",
    "\n",
    "\n",
    "val_targets_m = np.where(val_targets_m=='entailment',1,val_targets_m)\n",
    "val_targets_m = np.where(val_targets_m=='neutral',0,val_targets_m)\n",
    "val_targets_m = np.where(val_targets_m=='contradiction',2,val_targets_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = ['<pad>','<unk>']\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    words_ft['<pad>'] = PAD_IDX \n",
    "    words_ft['<unk>'] = UNK_IDX   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset of sentence1 size is 100000\n",
      "Train dataset of sentence2 size is 100000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words_ft[token] if token in words_ft else UNK_IDX for token in tokens.split(' ')]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_sentence1_indices = token2index_dataset(train_data['sentence1'])\n",
    "train_data_sentence2_indices = token2index_dataset(train_data['sentence2'])\n",
    "\n",
    "val_data_sentence1_indices = token2index_dataset(val_data['sentence1'])\n",
    "val_data_sentence2_indices = token2index_dataset(val_data['sentence2'])\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset of sentence1 size is {}\".format(len(train_data_sentence1_indices)))\n",
    "print (\"Train dataset of sentence2 size is {}\".format(len(train_data_sentence2_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SENTENCE_LENGTH = 80\n",
    "    \n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_1,data_list_2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list_1 = data_list_1\n",
    "        self.data_list_2 = data_list_2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list_1) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "\n",
    "        token_idx_1 = self.data_list_1[key]\n",
    "        token_idx_2 = self.data_list_2[key]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx_1,token_idx_2, len(token_idx_1),len(token_idx_2), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list_1 = []\n",
    "    length_list_2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list_1.append(datum[2])\n",
    "        #print(datum[2])\n",
    "        length_list_2.append(datum[3])\n",
    "        #print(datum[3])\n",
    "\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        \n",
    "        max_s1 = max (length_list_1) \n",
    "        max_s2 = max (length_list_2) \n",
    "\n",
    "\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_s1-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec_1)\n",
    "        \n",
    "        #print(data_list_1[0])\n",
    "        \n",
    "        padded_vec_2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_s2-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_2.append(padded_vec_2)\n",
    "        #print(data_list_2[0])\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list_1)),torch.from_numpy(np.array(data_list_2)),torch.from_numpy(np.array(length_list_1)), torch.LongTensor(length_list_2),torch.LongTensor(label_list)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1,data_2, lengths_1,lengths_2, labels in loader:\n",
    "        data_batch1,data_batch2, length_batch1,length_batch2, label_batch = data_1.cuda(),data_2.cuda(), lengths_1.cuda(),lengths_2.cuda(), labels.cuda()\n",
    "        outputs = F.softmax(model(data_batch1,data_batch2, length_batch1,length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        #print(outputs)\n",
    "        total += label_batch.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SNLI_RNN(nn.Module):\n",
    "    \"\"\"\n",
    "    SNLI classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings,vocab_size,emb_dim,hidden_size,num_layers,num_classes,FC_hiden):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(SNLI_RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float(), freeze=False, sparse=False)\n",
    "\n",
    "                \n",
    "\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size=emb_dim, hidden_size=hidden_size,num_layers=num_layers,batch_first=True,\n",
    "                        bidirectional=True)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            #nn.Linear(hidden_size,FC_hiden),\n",
    "            nn.Linear(hidden_size*2,FC_hiden),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(),\n",
    "            nn.Linear(FC_hiden,num_classes),\n",
    "            nn.ReLU())     \n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Function initializes the activation of recurrent neural net at timestep 0\n",
    "        # Needs to be in format (num_layers, batch_size, hidden_size)\n",
    "        hidden = torch.randn(self.num_layers*2, batch_size, self.hidden_size).cuda()\n",
    "\n",
    "        return hidden\n",
    "    \n",
    "    \n",
    "    def forward(self, data_1,data_2, length_1,length_2):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size_1, seq_len_1 = data_1.size()\n",
    "        batch_size_2, seq_len_2 = data_2.size()\n",
    "\n",
    "        hidden1 = self.init_hidden(batch_size_1)\n",
    "        hidden2 = self.init_hidden(batch_size_2)\n",
    "   \n",
    "        #S1 = self.embed(data_1)\n",
    "        #print(data_1.size())\n",
    "        S1 = self.embed(data_1)\n",
    "        m1 = (data_1 == 1)\n",
    "        #print(m1)\n",
    "        m1 = m1.unsqueeze(2).repeat(1, 1, self.emb_dim).float()\n",
    "        S1 = m1 * S1 + (1-m1) * S1.clone().detach()\n",
    "        \n",
    " \n",
    "        #print(S1.size())\n",
    "\n",
    "        S2 = self.embed(data_2)\n",
    "        m2 = (data_2 == 1)\n",
    "        m2 = m2.unsqueeze(2).repeat(1, 1, self.emb_dim).float()\n",
    "        S2 = m2 * S2 + (1-m2) * S2.clone().detach()\n",
    "\n",
    "        # sort the seqs in des order\n",
    "        _, idx_sort_1 = torch.sort(length_1, dim=0, descending=True)\n",
    "        _, idx_unsort_1 = torch.sort(idx_sort_1, dim=0)\n",
    "        S1 = S1.index_select(0, idx_sort_1)\n",
    "        length_1 = list(length_1[idx_sort_1])\n",
    "\n",
    "        _, idx_sort_2 = torch.sort(length_2, dim=0, descending=True)\n",
    "        _, idx_unsort_2 = torch.sort(idx_sort_2, dim=0)\n",
    "        S2 = S2.index_select(0, idx_sort_2)\n",
    "        length_2 = list(length_2[idx_sort_2])\n",
    "        \n",
    "        #print(\"batch size\",S1.size())\n",
    "\n",
    "        # transform pytorch tensor to padded sequence, pack padded sequence\n",
    "        S1 = torch.nn.utils.rnn.pack_padded_sequence(S1, np.array(length_1), batch_first=True)\n",
    "        S2 = torch.nn.utils.rnn.pack_padded_sequence(S2, np.array(length_2), batch_first=True)\n",
    "       \n",
    "        _,S1 = self.rnn(S1,hidden1)\n",
    "        _,S2 = self.rnn(S2,hidden2)\n",
    "        \n",
    "        #print(\"batch size\",S1.size())\n",
    "        \n",
    "        \n",
    "        S1 = torch.sum(S1,0)\n",
    "        S2 = torch.sum(S2,0)\n",
    "        \n",
    "        #print(\"batch size\",S2.size())\n",
    "\n",
    "        # Unsort\n",
    "        S1 = S1.index_select(0,idx_unsort_1)\n",
    "        S2 = S2.index_select(0,idx_unsort_2)\n",
    "\n",
    "        \n",
    "        #out = torch.mul(S1, S2)\n",
    "        out = torch.cat([S1, S2], 1)\n",
    "        #print(out.size())\n",
    "\n",
    "             \n",
    "        # return logits\n",
    "        out = self.linear(out)\n",
    "        #print(out.size())\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/30], Step: [100/1000], Validation Acc: 46.1\n",
      "Epoch: [1/30], Step: [199/1000], Validation Acc: 55.1\n",
      "Epoch: [1/30], Step: [298/1000], Validation Acc: 56.5\n",
      "Epoch: [1/30], Step: [397/1000], Validation Acc: 59.8\n",
      "Epoch: [1/30], Step: [496/1000], Validation Acc: 59.6\n",
      "Epoch: [1/30], Step: [595/1000], Validation Acc: 58.3\n",
      "Epoch: [1/30], Step: [694/1000], Validation Acc: 60.4\n",
      "Epoch: [1/30], Step: [793/1000], Validation Acc: 59.3\n",
      "Epoch: [1/30], Step: [892/1000], Validation Acc: 60.1\n",
      "Epoch: [1/30], Step: [991/1000], Validation Acc: 60.2\n",
      "Epoch: [2/30], Step: [100/1000], Validation Acc: 59.7\n",
      "Epoch: [2/30], Step: [199/1000], Validation Acc: 60.1\n",
      "Epoch: [2/30], Step: [298/1000], Validation Acc: 60.6\n",
      "Epoch: [2/30], Step: [397/1000], Validation Acc: 60.6\n",
      "Epoch: [2/30], Step: [496/1000], Validation Acc: 60.9\n",
      "Epoch: [2/30], Step: [595/1000], Validation Acc: 61.9\n",
      "Epoch: [2/30], Step: [694/1000], Validation Acc: 60.7\n",
      "Epoch: [2/30], Step: [793/1000], Validation Acc: 61.5\n",
      "Epoch: [2/30], Step: [892/1000], Validation Acc: 61.5\n",
      "Epoch: [2/30], Step: [991/1000], Validation Acc: 64.6\n",
      "Epoch: [3/30], Step: [100/1000], Validation Acc: 62.0\n",
      "Epoch: [3/30], Step: [199/1000], Validation Acc: 62.8\n",
      "Epoch: [3/30], Step: [298/1000], Validation Acc: 63.8\n",
      "Epoch: [3/30], Step: [397/1000], Validation Acc: 64.3\n",
      "Epoch: [3/30], Step: [496/1000], Validation Acc: 65.5\n",
      "Epoch: [3/30], Step: [595/1000], Validation Acc: 64.6\n",
      "Epoch: [3/30], Step: [694/1000], Validation Acc: 66.2\n",
      "Epoch: [3/30], Step: [793/1000], Validation Acc: 64.4\n",
      "Epoch: [3/30], Step: [892/1000], Validation Acc: 65.7\n",
      "Epoch: [3/30], Step: [991/1000], Validation Acc: 65.2\n",
      "Epoch: [4/30], Step: [100/1000], Validation Acc: 66.0\n",
      "Epoch: [4/30], Step: [199/1000], Validation Acc: 65.5\n",
      "Epoch: [4/30], Step: [298/1000], Validation Acc: 65.5\n",
      "Epoch: [4/30], Step: [397/1000], Validation Acc: 64.9\n",
      "Epoch: [4/30], Step: [496/1000], Validation Acc: 65.7\n",
      "Epoch: [4/30], Step: [595/1000], Validation Acc: 66.6\n",
      "Epoch: [4/30], Step: [694/1000], Validation Acc: 65.6\n",
      "Epoch: [4/30], Step: [793/1000], Validation Acc: 66.1\n",
      "Epoch: [4/30], Step: [892/1000], Validation Acc: 65.7\n",
      "Epoch: [4/30], Step: [991/1000], Validation Acc: 66.6\n",
      "Epoch: [5/30], Step: [100/1000], Validation Acc: 66.2\n",
      "Epoch: [5/30], Step: [199/1000], Validation Acc: 67.0\n",
      "Epoch: [5/30], Step: [298/1000], Validation Acc: 67.7\n",
      "Epoch: [5/30], Step: [397/1000], Validation Acc: 67.4\n",
      "Epoch: [5/30], Step: [496/1000], Validation Acc: 65.4\n",
      "Epoch: [5/30], Step: [595/1000], Validation Acc: 68.0\n",
      "Epoch: [5/30], Step: [694/1000], Validation Acc: 65.7\n",
      "Epoch: [5/30], Step: [793/1000], Validation Acc: 69.0\n",
      "Epoch: [5/30], Step: [892/1000], Validation Acc: 66.5\n",
      "Epoch: [5/30], Step: [991/1000], Validation Acc: 68.8\n",
      "Epoch: [6/30], Step: [100/1000], Validation Acc: 67.3\n",
      "Epoch: [6/30], Step: [199/1000], Validation Acc: 68.3\n",
      "Epoch: [6/30], Step: [298/1000], Validation Acc: 66.9\n",
      "Epoch: [6/30], Step: [397/1000], Validation Acc: 68.2\n",
      "Epoch: [6/30], Step: [496/1000], Validation Acc: 68.7\n",
      "Epoch: [6/30], Step: [595/1000], Validation Acc: 67.6\n",
      "Epoch: [6/30], Step: [694/1000], Validation Acc: 68.2\n",
      "Epoch: [6/30], Step: [793/1000], Validation Acc: 68.7\n",
      "Epoch: [6/30], Step: [892/1000], Validation Acc: 68.2\n",
      "Epoch: [6/30], Step: [991/1000], Validation Acc: 68.6\n",
      "Epoch: [7/30], Step: [100/1000], Validation Acc: 68.2\n",
      "Epoch: [7/30], Step: [199/1000], Validation Acc: 69.4\n",
      "Epoch: [7/30], Step: [298/1000], Validation Acc: 67.8\n",
      "Epoch: [7/30], Step: [397/1000], Validation Acc: 67.4\n",
      "Epoch: [7/30], Step: [496/1000], Validation Acc: 68.0\n",
      "Epoch: [7/30], Step: [595/1000], Validation Acc: 68.4\n",
      "Epoch: [7/30], Step: [694/1000], Validation Acc: 68.9\n",
      "Epoch: [7/30], Step: [793/1000], Validation Acc: 69.5\n",
      "Epoch: [7/30], Step: [892/1000], Validation Acc: 68.3\n",
      "Epoch: [7/30], Step: [991/1000], Validation Acc: 68.9\n",
      "Epoch: [8/30], Step: [100/1000], Validation Acc: 69.3\n",
      "Epoch: [8/30], Step: [199/1000], Validation Acc: 67.7\n",
      "Epoch: [8/30], Step: [298/1000], Validation Acc: 68.6\n",
      "Epoch: [8/30], Step: [397/1000], Validation Acc: 68.1\n",
      "Epoch: [8/30], Step: [496/1000], Validation Acc: 68.9\n",
      "Epoch: [8/30], Step: [595/1000], Validation Acc: 68.2\n",
      "Epoch: [8/30], Step: [694/1000], Validation Acc: 69.2\n",
      "Epoch: [8/30], Step: [793/1000], Validation Acc: 68.8\n",
      "Epoch: [8/30], Step: [892/1000], Validation Acc: 68.8\n",
      "Epoch: [8/30], Step: [991/1000], Validation Acc: 69.8\n",
      "Epoch: [9/30], Step: [100/1000], Validation Acc: 70.0\n",
      "Epoch: [9/30], Step: [199/1000], Validation Acc: 67.9\n",
      "Epoch: [9/30], Step: [298/1000], Validation Acc: 69.4\n",
      "Epoch: [9/30], Step: [397/1000], Validation Acc: 69.2\n",
      "Epoch: [9/30], Step: [496/1000], Validation Acc: 68.3\n",
      "Epoch: [9/30], Step: [595/1000], Validation Acc: 67.3\n",
      "Epoch: [9/30], Step: [694/1000], Validation Acc: 70.3\n",
      "Epoch: [9/30], Step: [793/1000], Validation Acc: 68.5\n",
      "Epoch: [9/30], Step: [892/1000], Validation Acc: 69.6\n",
      "Epoch: [9/30], Step: [991/1000], Validation Acc: 69.3\n",
      "Epoch: [10/30], Step: [100/1000], Validation Acc: 69.5\n",
      "Epoch: [10/30], Step: [199/1000], Validation Acc: 69.0\n",
      "Epoch: [10/30], Step: [298/1000], Validation Acc: 68.8\n",
      "Epoch: [10/30], Step: [397/1000], Validation Acc: 69.3\n",
      "Epoch: [10/30], Step: [496/1000], Validation Acc: 68.3\n",
      "Epoch: [10/30], Step: [595/1000], Validation Acc: 69.4\n",
      "Epoch: [10/30], Step: [694/1000], Validation Acc: 70.2\n",
      "Epoch: [10/30], Step: [793/1000], Validation Acc: 70.0\n",
      "Epoch: [10/30], Step: [892/1000], Validation Acc: 70.7\n",
      "Epoch: [10/30], Step: [991/1000], Validation Acc: 69.7\n",
      "Epoch: [11/30], Step: [100/1000], Validation Acc: 68.8\n",
      "Epoch: [11/30], Step: [199/1000], Validation Acc: 68.8\n",
      "Epoch: [11/30], Step: [298/1000], Validation Acc: 70.3\n",
      "Epoch: [11/30], Step: [397/1000], Validation Acc: 69.2\n",
      "Epoch: [11/30], Step: [496/1000], Validation Acc: 70.2\n",
      "Epoch: [11/30], Step: [595/1000], Validation Acc: 69.0\n",
      "Epoch: [11/30], Step: [694/1000], Validation Acc: 70.3\n",
      "Epoch: [11/30], Step: [793/1000], Validation Acc: 69.6\n",
      "Epoch: [11/30], Step: [892/1000], Validation Acc: 70.3\n",
      "Epoch: [11/30], Step: [991/1000], Validation Acc: 69.3\n",
      "Epoch: [12/30], Step: [100/1000], Validation Acc: 68.7\n",
      "Epoch: [12/30], Step: [199/1000], Validation Acc: 70.4\n",
      "Epoch: [12/30], Step: [298/1000], Validation Acc: 70.4\n",
      "Epoch: [12/30], Step: [397/1000], Validation Acc: 69.3\n",
      "Epoch: [12/30], Step: [496/1000], Validation Acc: 69.4\n",
      "Epoch: [12/30], Step: [595/1000], Validation Acc: 69.8\n",
      "Epoch: [12/30], Step: [694/1000], Validation Acc: 69.3\n",
      "Epoch: [12/30], Step: [793/1000], Validation Acc: 70.4\n",
      "Epoch: [12/30], Step: [892/1000], Validation Acc: 69.6\n",
      "Epoch: [12/30], Step: [991/1000], Validation Acc: 70.2\n",
      "Epoch: [13/30], Step: [100/1000], Validation Acc: 69.0\n",
      "Epoch: [13/30], Step: [199/1000], Validation Acc: 69.9\n",
      "Epoch: [13/30], Step: [298/1000], Validation Acc: 70.3\n",
      "Epoch: [13/30], Step: [397/1000], Validation Acc: 71.2\n",
      "Epoch: [13/30], Step: [496/1000], Validation Acc: 68.3\n",
      "Epoch: [13/30], Step: [595/1000], Validation Acc: 70.5\n",
      "Epoch: [13/30], Step: [694/1000], Validation Acc: 69.2\n",
      "Epoch: [13/30], Step: [793/1000], Validation Acc: 69.9\n",
      "Epoch: [13/30], Step: [892/1000], Validation Acc: 69.4\n",
      "Epoch: [13/30], Step: [991/1000], Validation Acc: 70.0\n",
      "Epoch: [14/30], Step: [100/1000], Validation Acc: 69.8\n",
      "Epoch: [14/30], Step: [199/1000], Validation Acc: 69.8\n",
      "Epoch: [14/30], Step: [298/1000], Validation Acc: 69.4\n",
      "Epoch: [14/30], Step: [397/1000], Validation Acc: 69.1\n",
      "Epoch: [14/30], Step: [496/1000], Validation Acc: 68.7\n",
      "Epoch: [14/30], Step: [595/1000], Validation Acc: 70.4\n",
      "Epoch: [14/30], Step: [694/1000], Validation Acc: 69.8\n",
      "Epoch: [14/30], Step: [793/1000], Validation Acc: 69.4\n",
      "Epoch: [14/30], Step: [892/1000], Validation Acc: 70.0\n",
      "Epoch: [14/30], Step: [991/1000], Validation Acc: 70.6\n",
      "Epoch: [15/30], Step: [100/1000], Validation Acc: 69.9\n",
      "Epoch: [15/30], Step: [199/1000], Validation Acc: 69.5\n",
      "Epoch: [15/30], Step: [298/1000], Validation Acc: 71.6\n",
      "Epoch: [15/30], Step: [397/1000], Validation Acc: 70.2\n",
      "Epoch: [15/30], Step: [496/1000], Validation Acc: 69.5\n",
      "Epoch: [15/30], Step: [595/1000], Validation Acc: 70.0\n",
      "Epoch: [15/30], Step: [694/1000], Validation Acc: 70.3\n",
      "Epoch: [15/30], Step: [793/1000], Validation Acc: 69.8\n",
      "Epoch: [15/30], Step: [892/1000], Validation Acc: 70.1\n",
      "Epoch: [15/30], Step: [991/1000], Validation Acc: 70.8\n",
      "Epoch: [16/30], Step: [100/1000], Validation Acc: 70.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [16/30], Step: [199/1000], Validation Acc: 68.9\n",
      "Epoch: [16/30], Step: [298/1000], Validation Acc: 70.4\n",
      "Epoch: [16/30], Step: [397/1000], Validation Acc: 69.0\n",
      "Epoch: [16/30], Step: [496/1000], Validation Acc: 69.6\n",
      "Epoch: [16/30], Step: [595/1000], Validation Acc: 69.5\n",
      "Epoch: [16/30], Step: [694/1000], Validation Acc: 70.2\n",
      "Epoch: [16/30], Step: [793/1000], Validation Acc: 69.9\n",
      "Epoch: [16/30], Step: [892/1000], Validation Acc: 68.5\n",
      "Epoch: [16/30], Step: [991/1000], Validation Acc: 70.7\n",
      "Epoch: [17/30], Step: [100/1000], Validation Acc: 69.7\n",
      "Epoch: [17/30], Step: [199/1000], Validation Acc: 71.0\n",
      "Epoch: [17/30], Step: [298/1000], Validation Acc: 69.1\n",
      "Epoch: [17/30], Step: [397/1000], Validation Acc: 69.7\n",
      "Epoch: [17/30], Step: [496/1000], Validation Acc: 69.6\n",
      "Epoch: [17/30], Step: [595/1000], Validation Acc: 70.3\n",
      "Epoch: [17/30], Step: [694/1000], Validation Acc: 70.9\n",
      "Epoch: [17/30], Step: [793/1000], Validation Acc: 68.9\n",
      "Epoch: [17/30], Step: [892/1000], Validation Acc: 69.3\n",
      "Epoch: [17/30], Step: [991/1000], Validation Acc: 69.5\n",
      "Epoch: [18/30], Step: [100/1000], Validation Acc: 69.5\n",
      "Epoch: [18/30], Step: [199/1000], Validation Acc: 68.9\n",
      "Epoch: [18/30], Step: [298/1000], Validation Acc: 67.8\n",
      "Epoch: [18/30], Step: [397/1000], Validation Acc: 68.3\n",
      "Epoch: [18/30], Step: [496/1000], Validation Acc: 69.2\n",
      "Epoch: [18/30], Step: [595/1000], Validation Acc: 69.7\n",
      "Epoch: [18/30], Step: [694/1000], Validation Acc: 69.0\n",
      "Epoch: [18/30], Step: [793/1000], Validation Acc: 69.7\n",
      "Epoch: [18/30], Step: [892/1000], Validation Acc: 68.1\n",
      "Epoch: [18/30], Step: [991/1000], Validation Acc: 68.8\n",
      "Epoch: [19/30], Step: [100/1000], Validation Acc: 69.2\n",
      "Epoch: [19/30], Step: [199/1000], Validation Acc: 68.9\n",
      "Epoch: [19/30], Step: [298/1000], Validation Acc: 68.7\n",
      "Epoch: [19/30], Step: [397/1000], Validation Acc: 69.2\n",
      "Epoch: [19/30], Step: [496/1000], Validation Acc: 69.9\n",
      "Epoch: [19/30], Step: [595/1000], Validation Acc: 69.8\n",
      "Epoch: [19/30], Step: [694/1000], Validation Acc: 68.6\n",
      "Epoch: [19/30], Step: [793/1000], Validation Acc: 68.7\n",
      "Epoch: [19/30], Step: [892/1000], Validation Acc: 69.2\n",
      "Epoch: [19/30], Step: [991/1000], Validation Acc: 69.2\n",
      "Epoch: [20/30], Step: [100/1000], Validation Acc: 67.8\n",
      "Epoch: [20/30], Step: [199/1000], Validation Acc: 67.0\n",
      "Epoch: [20/30], Step: [298/1000], Validation Acc: 69.8\n",
      "Epoch: [20/30], Step: [397/1000], Validation Acc: 69.6\n",
      "Epoch: [20/30], Step: [496/1000], Validation Acc: 69.8\n",
      "Epoch: [20/30], Step: [595/1000], Validation Acc: 69.0\n",
      "Epoch: [20/30], Step: [694/1000], Validation Acc: 69.5\n",
      "Epoch: [20/30], Step: [793/1000], Validation Acc: 69.8\n",
      "Epoch: [20/30], Step: [892/1000], Validation Acc: 68.9\n",
      "Epoch: [20/30], Step: [991/1000], Validation Acc: 67.9\n",
      "Epoch: [21/30], Step: [100/1000], Validation Acc: 68.6\n",
      "Epoch: [21/30], Step: [199/1000], Validation Acc: 68.3\n",
      "Epoch: [21/30], Step: [298/1000], Validation Acc: 68.7\n",
      "Epoch: [21/30], Step: [397/1000], Validation Acc: 69.3\n",
      "Epoch: [21/30], Step: [496/1000], Validation Acc: 68.4\n",
      "Epoch: [21/30], Step: [595/1000], Validation Acc: 68.7\n",
      "Epoch: [21/30], Step: [694/1000], Validation Acc: 69.4\n",
      "Epoch: [21/30], Step: [793/1000], Validation Acc: 67.8\n",
      "Epoch: [21/30], Step: [892/1000], Validation Acc: 69.3\n",
      "Epoch: [21/30], Step: [991/1000], Validation Acc: 69.8\n",
      "Epoch: [22/30], Step: [100/1000], Validation Acc: 69.4\n",
      "Epoch: [22/30], Step: [199/1000], Validation Acc: 68.6\n",
      "Epoch: [22/30], Step: [298/1000], Validation Acc: 70.5\n",
      "Epoch: [22/30], Step: [397/1000], Validation Acc: 69.0\n",
      "Epoch: [22/30], Step: [496/1000], Validation Acc: 68.1\n",
      "Epoch: [22/30], Step: [595/1000], Validation Acc: 69.5\n",
      "Epoch: [22/30], Step: [694/1000], Validation Acc: 68.5\n",
      "Epoch: [22/30], Step: [793/1000], Validation Acc: 68.5\n",
      "Epoch: [22/30], Step: [892/1000], Validation Acc: 69.3\n",
      "Epoch: [22/30], Step: [991/1000], Validation Acc: 68.5\n",
      "Epoch: [23/30], Step: [100/1000], Validation Acc: 68.0\n",
      "Epoch: [23/30], Step: [199/1000], Validation Acc: 68.5\n",
      "Epoch: [23/30], Step: [298/1000], Validation Acc: 67.5\n",
      "Epoch: [23/30], Step: [397/1000], Validation Acc: 68.2\n",
      "Epoch: [23/30], Step: [496/1000], Validation Acc: 68.6\n",
      "Epoch: [23/30], Step: [595/1000], Validation Acc: 69.1\n",
      "Epoch: [23/30], Step: [694/1000], Validation Acc: 67.9\n",
      "Epoch: [23/30], Step: [793/1000], Validation Acc: 68.3\n",
      "Epoch: [23/30], Step: [892/1000], Validation Acc: 68.5\n",
      "Epoch: [23/30], Step: [991/1000], Validation Acc: 69.2\n",
      "Epoch: [24/30], Step: [100/1000], Validation Acc: 68.3\n",
      "Epoch: [24/30], Step: [199/1000], Validation Acc: 67.8\n",
      "Epoch: [24/30], Step: [298/1000], Validation Acc: 68.6\n",
      "Epoch: [24/30], Step: [397/1000], Validation Acc: 67.4\n",
      "Epoch: [24/30], Step: [496/1000], Validation Acc: 68.2\n",
      "Epoch: [24/30], Step: [595/1000], Validation Acc: 69.0\n",
      "Epoch: [24/30], Step: [694/1000], Validation Acc: 69.1\n",
      "Epoch: [24/30], Step: [793/1000], Validation Acc: 68.0\n",
      "Epoch: [24/30], Step: [892/1000], Validation Acc: 67.7\n",
      "Epoch: [24/30], Step: [991/1000], Validation Acc: 68.5\n",
      "Epoch: [25/30], Step: [100/1000], Validation Acc: 68.8\n",
      "Epoch: [25/30], Step: [199/1000], Validation Acc: 69.6\n",
      "Epoch: [25/30], Step: [298/1000], Validation Acc: 69.2\n",
      "Epoch: [25/30], Step: [397/1000], Validation Acc: 68.8\n",
      "Epoch: [25/30], Step: [496/1000], Validation Acc: 68.2\n",
      "Epoch: [25/30], Step: [595/1000], Validation Acc: 67.9\n",
      "Epoch: [25/30], Step: [694/1000], Validation Acc: 68.5\n",
      "Epoch: [25/30], Step: [793/1000], Validation Acc: 68.7\n",
      "Epoch: [25/30], Step: [892/1000], Validation Acc: 67.5\n",
      "Epoch: [25/30], Step: [991/1000], Validation Acc: 68.3\n",
      "Epoch: [26/30], Step: [100/1000], Validation Acc: 68.5\n",
      "Epoch: [26/30], Step: [199/1000], Validation Acc: 67.3\n",
      "Epoch: [26/30], Step: [298/1000], Validation Acc: 67.3\n",
      "Epoch: [26/30], Step: [397/1000], Validation Acc: 67.5\n",
      "Epoch: [26/30], Step: [496/1000], Validation Acc: 67.9\n",
      "Epoch: [26/30], Step: [595/1000], Validation Acc: 67.6\n",
      "Epoch: [26/30], Step: [694/1000], Validation Acc: 68.2\n",
      "Epoch: [26/30], Step: [793/1000], Validation Acc: 68.9\n",
      "Epoch: [26/30], Step: [892/1000], Validation Acc: 68.2\n",
      "Epoch: [26/30], Step: [991/1000], Validation Acc: 68.5\n",
      "Epoch: [27/30], Step: [100/1000], Validation Acc: 67.9\n",
      "Epoch: [27/30], Step: [199/1000], Validation Acc: 68.6\n",
      "Epoch: [27/30], Step: [298/1000], Validation Acc: 68.2\n",
      "Epoch: [27/30], Step: [397/1000], Validation Acc: 67.0\n",
      "Epoch: [27/30], Step: [496/1000], Validation Acc: 67.8\n",
      "Epoch: [27/30], Step: [595/1000], Validation Acc: 67.6\n",
      "Epoch: [27/30], Step: [694/1000], Validation Acc: 67.8\n",
      "Epoch: [27/30], Step: [793/1000], Validation Acc: 67.1\n",
      "Epoch: [27/30], Step: [892/1000], Validation Acc: 68.8\n",
      "Epoch: [27/30], Step: [991/1000], Validation Acc: 68.6\n",
      "Epoch: [28/30], Step: [100/1000], Validation Acc: 68.1\n",
      "Epoch: [28/30], Step: [199/1000], Validation Acc: 68.3\n",
      "Epoch: [28/30], Step: [298/1000], Validation Acc: 67.5\n",
      "Epoch: [28/30], Step: [397/1000], Validation Acc: 67.4\n",
      "Epoch: [28/30], Step: [496/1000], Validation Acc: 67.0\n",
      "Epoch: [28/30], Step: [595/1000], Validation Acc: 66.1\n",
      "Epoch: [28/30], Step: [694/1000], Validation Acc: 66.8\n",
      "Epoch: [28/30], Step: [793/1000], Validation Acc: 68.2\n",
      "Epoch: [28/30], Step: [892/1000], Validation Acc: 68.2\n",
      "Epoch: [28/30], Step: [991/1000], Validation Acc: 69.4\n",
      "Epoch: [29/30], Step: [100/1000], Validation Acc: 67.4\n",
      "Epoch: [29/30], Step: [199/1000], Validation Acc: 68.4\n",
      "Epoch: [29/30], Step: [298/1000], Validation Acc: 67.6\n",
      "Epoch: [29/30], Step: [397/1000], Validation Acc: 68.6\n",
      "Epoch: [29/30], Step: [496/1000], Validation Acc: 68.3\n",
      "Epoch: [29/30], Step: [595/1000], Validation Acc: 67.9\n",
      "Epoch: [29/30], Step: [694/1000], Validation Acc: 69.5\n",
      "Epoch: [29/30], Step: [793/1000], Validation Acc: 68.5\n",
      "Epoch: [29/30], Step: [892/1000], Validation Acc: 67.8\n",
      "Epoch: [29/30], Step: [991/1000], Validation Acc: 68.9\n",
      "Epoch: [30/30], Step: [100/1000], Validation Acc: 67.8\n",
      "Epoch: [30/30], Step: [199/1000], Validation Acc: 68.6\n",
      "Epoch: [30/30], Step: [298/1000], Validation Acc: 68.9\n",
      "Epoch: [30/30], Step: [397/1000], Validation Acc: 68.5\n",
      "Epoch: [30/30], Step: [496/1000], Validation Acc: 68.2\n",
      "Epoch: [30/30], Step: [595/1000], Validation Acc: 68.2\n",
      "Epoch: [30/30], Step: [694/1000], Validation Acc: 67.8\n",
      "Epoch: [30/30], Step: [793/1000], Validation Acc: 68.3\n",
      "Epoch: [30/30], Step: [892/1000], Validation Acc: 66.9\n",
      "Epoch: [30/30], Step: [991/1000], Validation Acc: 67.7\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_dataset = NewsGroupDataset(train_data_sentence1_indices,train_data_sentence2_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_sentence1_indices,val_data_sentence2_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "# Train the best RNN model\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 20 # number epoch to train\n",
    "\n",
    "model = SNLI_RNN(loaded_embeddings_ft,50000,300,250,num_layers=1,num_classes=3,FC_hiden=50).cuda()\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss=[]\n",
    "    val_acc = 0\n",
    "    tra_acc = 0\n",
    "    for i, (data_1,data_2, lengths_1,lengths_2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch1,data_batch2,length_batch1,length_batch2,label_batch = data_1.cuda(),data_2.cuda(), lengths_1.cuda(),lengths_2.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch1,data_batch2, length_batch1,length_batch2)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        epoch_loss.append(loss)\n",
    "        optimizer.step()\n",
    "        # validate every 32 iterations\n",
    "        if i > 0 and i % (BATCH_SIZE-1) == 0:\n",
    "            # validate\n",
    "            tra_acc = test_model(train_loader, model)\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                        epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Without tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('telephone', 48.95522388059702), ('fiction', 45.82914572864322), ('slate', 44.41117764471058), ('government', 45.17716535433071), ('travel', 45.21384928716904)]\n"
     ]
    }
   ],
   "source": [
    "res_without_tun = []\n",
    "for g in train_data_m['genre'].unique():\n",
    "    \n",
    "    val_data_sentence1_indices = token2index_dataset(val_data_m[val_data_m['genre']==g]['sentence1'])\n",
    "    val_data_sentence2_indices = token2index_dataset(val_data_m[val_data_m['genre']==g]['sentence2'])\n",
    "    val_targets_g = val_targets_m[val_data_m['genre']==g]\n",
    "\n",
    "    val_dataset = NewsGroupDataset(val_data_sentence1_indices,val_data_sentence2_indices, val_targets_g)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    \n",
    "    val_acc = test_model(val_loader, model)\n",
    "    \n",
    "    \n",
    "    res_without_tun.append((g,val_acc))\n",
    "\n",
    "print(res_without_tun)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### With tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"B_rnn_model_states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "telephone\n",
      "Epoch: [1/1], Step: [20/214], Validation Acc: 45.67164179104478\n",
      "Epoch: [1/1], Step: [39/214], Validation Acc: 47.06467661691542\n",
      "Epoch: [1/1], Step: [58/214], Validation Acc: 47.06467661691542\n",
      "Epoch: [1/1], Step: [77/214], Validation Acc: 46.26865671641791\n",
      "Epoch: [1/1], Step: [96/214], Validation Acc: 45.87064676616915\n",
      "Epoch: [1/1], Step: [115/214], Validation Acc: 46.26865671641791\n",
      "Epoch: [1/1], Step: [134/214], Validation Acc: 45.27363184079602\n",
      "Epoch: [1/1], Step: [153/214], Validation Acc: 46.169154228855724\n",
      "Epoch: [1/1], Step: [172/214], Validation Acc: 46.069651741293534\n",
      "Epoch: [1/1], Step: [191/214], Validation Acc: 45.87064676616915\n",
      "Epoch: [1/1], Step: [210/214], Validation Acc: 46.56716417910448\n",
      "fiction\n",
      "Epoch: [1/1], Step: [20/192], Validation Acc: 47.93969849246231\n",
      "Epoch: [1/1], Step: [39/192], Validation Acc: 45.82914572864322\n",
      "Epoch: [1/1], Step: [58/192], Validation Acc: 46.33165829145729\n",
      "Epoch: [1/1], Step: [77/192], Validation Acc: 47.8391959798995\n",
      "Epoch: [1/1], Step: [96/192], Validation Acc: 46.63316582914573\n",
      "Epoch: [1/1], Step: [115/192], Validation Acc: 47.537688442211056\n",
      "Epoch: [1/1], Step: [134/192], Validation Acc: 45.7286432160804\n",
      "Epoch: [1/1], Step: [153/192], Validation Acc: 46.53266331658291\n",
      "Epoch: [1/1], Step: [172/192], Validation Acc: 45.62814070351759\n",
      "Epoch: [1/1], Step: [191/192], Validation Acc: 45.7286432160804\n",
      "slate\n",
      "Epoch: [1/1], Step: [20/202], Validation Acc: 43.51297405189621\n",
      "Epoch: [1/1], Step: [39/202], Validation Acc: 44.61077844311377\n",
      "Epoch: [1/1], Step: [58/202], Validation Acc: 43.712574850299404\n",
      "Epoch: [1/1], Step: [77/202], Validation Acc: 45.00998003992016\n",
      "Epoch: [1/1], Step: [96/202], Validation Acc: 46.40718562874252\n",
      "Epoch: [1/1], Step: [115/202], Validation Acc: 45.50898203592814\n",
      "Epoch: [1/1], Step: [134/202], Validation Acc: 44.91017964071856\n",
      "Epoch: [1/1], Step: [153/202], Validation Acc: 44.71057884231537\n",
      "Epoch: [1/1], Step: [172/202], Validation Acc: 45.30938123752495\n",
      "Epoch: [1/1], Step: [191/202], Validation Acc: 44.11177644710579\n",
      "government\n",
      "Epoch: [1/1], Step: [20/195], Validation Acc: 45.07874015748032\n",
      "Epoch: [1/1], Step: [39/195], Validation Acc: 45.07874015748032\n",
      "Epoch: [1/1], Step: [58/195], Validation Acc: 45.47244094488189\n",
      "Epoch: [1/1], Step: [77/195], Validation Acc: 44.488188976377955\n",
      "Epoch: [1/1], Step: [96/195], Validation Acc: 45.669291338582674\n",
      "Epoch: [1/1], Step: [115/195], Validation Acc: 45.767716535433074\n",
      "Epoch: [1/1], Step: [134/195], Validation Acc: 44.488188976377955\n",
      "Epoch: [1/1], Step: [153/195], Validation Acc: 44.78346456692913\n",
      "Epoch: [1/1], Step: [172/195], Validation Acc: 44.68503937007874\n",
      "Epoch: [1/1], Step: [191/195], Validation Acc: 46.25984251968504\n",
      "travel\n",
      "Epoch: [1/1], Step: [20/200], Validation Acc: 44.29735234215886\n",
      "Epoch: [1/1], Step: [39/200], Validation Acc: 44.29735234215886\n",
      "Epoch: [1/1], Step: [58/200], Validation Acc: 44.60285132382892\n",
      "Epoch: [1/1], Step: [77/200], Validation Acc: 45.010183299389\n",
      "Epoch: [1/1], Step: [96/200], Validation Acc: 43.38085539714868\n",
      "Epoch: [1/1], Step: [115/200], Validation Acc: 44.19551934826884\n",
      "Epoch: [1/1], Step: [134/200], Validation Acc: 45.11201629327902\n",
      "Epoch: [1/1], Step: [153/200], Validation Acc: 45.11201629327902\n",
      "Epoch: [1/1], Step: [172/200], Validation Acc: 45.41751527494908\n",
      "Epoch: [1/1], Step: [191/200], Validation Acc: 43.9918533604888\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.00001\n",
    "num_epochs = 1 # number epoch to train\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "res_with_tun = []\n",
    "for g in train_data_m['genre'].unique():\n",
    "    \n",
    "    print(g)\n",
    "    \n",
    "    train_data_sentence1_indices_m = token2index_dataset(train_data_m[train_data_m['genre']==g]['sentence1'])\n",
    "    train_data_sentence2_indices_m = token2index_dataset(train_data_m[train_data_m['genre']==g]['sentence2'])\n",
    "    train_targets_g_m = train_targets_m[train_data_m['genre']==g]\n",
    "    \n",
    "    \n",
    "    val_data_sentence1_indices_m = token2index_dataset(val_data_m[val_data_m['genre']==g]['sentence1'])\n",
    "    val_data_sentence2_indices_m = token2index_dataset(val_data_m[val_data_m['genre']==g]['sentence2'])\n",
    "    val_targets_g_m = val_targets_m[val_data_m['genre']==g]\n",
    "    \n",
    "    train_dataset_m = NewsGroupDataset(train_data_sentence1_indices_m,train_data_sentence2_indices_m, train_targets_g_m)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset_m, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    val_dataset_m = NewsGroupDataset(val_data_sentence1_indices_m,val_data_sentence2_indices_m, val_targets_g_m)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset_m, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model_clone = SNLI_RNN(loaded_embeddings_ft,50000,300,250,num_layers=1,num_classes=3,FC_hiden=50).cuda()\n",
    "        model_clone.load_state_dict(torch.load(\"B_rnn_model_states\"))\n",
    "            # Criterion and Optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss()  \n",
    "        optimizer = torch.optim.Adam(model_clone.parameters(), lr=learning_rate)\n",
    "\n",
    "        epoch_loss=[]\n",
    "        val_acc = 0\n",
    "        tra_acc = 0\n",
    "        for i, (data_1,data_2, lengths_1,lengths_2, labels) in enumerate(train_loader):\n",
    "\n",
    "            model_clone.train()\n",
    "            \n",
    "            data_batch1,data_batch2,length_batch1,length_batch2,label_batch = data_1.cuda(),data_2.cuda(), lengths_1.cuda(),lengths_2.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_clone(data_batch1,data_batch2, length_batch1,length_batch2)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            loss.backward()\n",
    "            epoch_loss.append(loss)\n",
    "            optimizer.step()\n",
    "            # validate every 32 iterations\n",
    "            if i > 0 and i % (BATCH_SIZE-1) == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model_clone)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                            epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "    res_with_tun.append((g,val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('telephone', 46.56716417910448), ('fiction', 45.7286432160804), ('slate', 44.11177644710579), ('government', 46.25984251968504), ('travel', 43.9918533604888)]\n"
     ]
    }
   ],
   "source": [
    "print(res_with_tun)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
