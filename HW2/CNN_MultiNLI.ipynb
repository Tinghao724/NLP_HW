{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('hw2_data/snli_train.tsv',delimiter='\\t')\n",
    "val = pd.read_csv(\"hw2_data/snli_val.tsv\",delimiter='\\t')\n",
    "test = pd.read_csv(\"hw2_data/mnli_val.tsv\",delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[['sentence1','sentence2']]\n",
    "train_targets = np.array(train['label'])\n",
    "\n",
    "val_data = val[['sentence1','sentence2']]\n",
    "val_targets = np.array(val['label'])\n",
    "\n",
    "test_data = test[['sentence1','sentence2','genre']]\n",
    "test_targets = np.array(test['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = np.where(train_targets=='entailment',1,train_targets)\n",
    "train_targets = np.where(train_targets=='neutral',0,train_targets)\n",
    "train_targets = np.where(train_targets=='contradiction',2,train_targets)\n",
    "\n",
    "\n",
    "val_targets = np.where(val_targets=='entailment',1,val_targets)\n",
    "val_targets = np.where(val_targets=='neutral',0,val_targets)\n",
    "val_targets = np.where(val_targets=='contradiction',2,val_targets)\n",
    "\n",
    "test_targets = np.where(test_targets=='entailment',1,test_targets)\n",
    "test_targets = np.where(test_targets=='neutral',0,test_targets)\n",
    "test_targets = np.where(test_targets=='contradiction',2,test_targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_home = './'\n",
    "words_to_load = 50000\n",
    "\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "\n",
    "with open(ft_home + 'wiki-news-300d-1M.vec') as f:\n",
    "    loaded_embeddings_ft = np.zeros((words_to_load+2, 300))\n",
    "    words_ft = {}\n",
    "    idx2words_ft = {}\n",
    "    ordered_words_ft = ['<pad>','<unk>']\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        s = line.split()\n",
    "        loaded_embeddings_ft[i+2, :] = np.asarray(s[1:])\n",
    "        words_ft[s[0]] = i+2\n",
    "        idx2words_ft[i+2] = s[0]\n",
    "        ordered_words_ft.append(s[0])\n",
    "    words_ft['<pad>'] = PAD_IDX \n",
    "    words_ft['<unk>'] = UNK_IDX   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset of sentence1 size is 100000\n",
      "Train dataset of sentence2 size is 100000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [words_ft[token] if token in words_ft else UNK_IDX for token in tokens.split(' ')]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_sentence1_indices = token2index_dataset(train_data['sentence1'])\n",
    "train_data_sentence2_indices = token2index_dataset(train_data['sentence2'])\n",
    "\n",
    "val_data_sentence1_indices = token2index_dataset(val_data['sentence1'])\n",
    "val_data_sentence2_indices = token2index_dataset(val_data['sentence2'])\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset of sentence1 size is {}\".format(len(train_data_sentence1_indices)))\n",
    "print (\"Train dataset of sentence2 size is {}\".format(len(train_data_sentence2_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAX_SENTENCE_LENGTH = 80\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list_1,data_list_2, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list_1 = data_list_1\n",
    "        self.data_list_2 = data_list_2\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list_1) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "\n",
    "        token_idx_1 = self.data_list_1[key]\n",
    "        token_idx_2 = self.data_list_2[key]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx_1,token_idx_2, len(token_idx_1),len(token_idx_2), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list_1 = []\n",
    "    data_list_2 = []\n",
    "    label_list = []\n",
    "    length_list_1 = []\n",
    "    length_list_2 = []\n",
    "\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[4])\n",
    "        length_list_1.append(datum[2])\n",
    "        #print(datum[2])\n",
    "        length_list_2.append(datum[3])\n",
    "        #print(datum[3])\n",
    "\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        \n",
    "        max_s1 = max (length_list_1) \n",
    "        max_s2 = max (length_list_2) \n",
    "\n",
    "\n",
    "        padded_vec_1 = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,max_s1-datum[2])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_1.append(padded_vec_1)\n",
    "        \n",
    "        #print(data_list_1[0])\n",
    "        \n",
    "        padded_vec_2 = np.pad(np.array(datum[1]), \n",
    "                                pad_width=((0,max_s2-datum[3])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list_2.append(padded_vec_2)\n",
    "        #print(data_list_2[0])\n",
    "        \n",
    "    return [torch.from_numpy(np.array(data_list_1)),torch.from_numpy(np.array(data_list_2)),torch.from_numpy(np.array(length_list_1)), torch.LongTensor(length_list_2),torch.LongTensor(label_list)]\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_dataset = NewsGroupDataset(train_data_sentence1_indices,train_data_sentence2_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_sentence1_indices,val_data_sentence2_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SNLI_CNN(nn.Module):\n",
    "    def __init__(self,embeddings, emb_size, hidden_size, num_layers, num_classes, vocab_size=50000,kernel_size=2,FC_hiden=50):\n",
    "\n",
    "        super(SNLI_CNN, self).__init__()\n",
    "\n",
    "        self.num_layers, self.hidden_size = num_layers, hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.embed = nn.Embedding.from_pretrained(torch.from_numpy(embeddings).float(), freeze=False, sparse=False)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(emb_size, hidden_size, kernel_size, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_size, hidden_size, kernel_size, padding=1)\n",
    "\n",
    "                \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2,FC_hiden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(FC_hiden,num_classes),\n",
    "            nn.ReLU())     \n",
    "        \n",
    "    def forward(self, data_1,data_2,length_1,length_2):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size_1, seq_len_1 = data_1.size()\n",
    "        batch_size_2, seq_len_2 = data_2.size()\n",
    "\n",
    "        S1 = self.embed(data_1)\n",
    "        m1 = (data_1 == 1)\n",
    "        m1 = m1.unsqueeze(2).repeat(1, 1, self.emb_size).float()\n",
    "        S1 = m1 * S1 + (1-m1) * S1.clone().detach()\n",
    "        \n",
    " \n",
    "\n",
    "        S2 = self.embed(data_2)\n",
    "        m2 = (data_2 == 1)\n",
    "        m2 = m2.unsqueeze(2).repeat(1, 1, self.emb_size).float()\n",
    "        S2 = m2 * S2 + (1-m2) * S2.clone().detach()\n",
    "\n",
    "        \n",
    "        S1 = self.conv1(S1.transpose(1,2)).transpose(1,2)\n",
    "        S1 = F.relu(S1.contiguous().view(-1, S1.size(-1))).view(BATCH_SIZE, S1.size(1), S1.size(-1))\n",
    "\n",
    "\n",
    "        S1 = self.conv2(S1.transpose(1,2)).transpose(1,2)\n",
    "        S1 = F.relu(S1.contiguous().view(-1, S1.size(-1))).view(BATCH_SIZE, S1.size(1), S1.size(-1))\n",
    "        \n",
    "        S1 = torch.max(S1, dim=1)\n",
    "\n",
    "        S2 = self.conv1(S2.transpose(1,2)).transpose(1,2)\n",
    "        S2 = F.relu(S2.contiguous().view(-1, S2.size(-1))).view(BATCH_SIZE, S2.size(1), S2.size(-1))\n",
    "\n",
    "        S2 = self.conv2(S2.transpose(1,2)).transpose(1,2)\n",
    "        S2 = F.relu(S2.contiguous().view(-1, S2.size(-1))).view(BATCH_SIZE, S2.size(1), S2.size(-1))\n",
    "        S2 = torch.max(S2, dim=1)\n",
    "        \n",
    "        out = torch.cat([S1[0], S2[0]], 1)\n",
    "        #print(out.size())\n",
    "        \n",
    "        logits = self.linear(out)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data_1,data_2, lengths_1,lengths_2, labels in loader:\n",
    "        data_batch1,data_batch2, length_batch1,length_batch2, label_batch = data_1.cuda(),data_2.cuda(), lengths_1.cuda(),lengths_2.cuda(), labels.cuda()\n",
    "        outputs = F.softmax(model(data_batch1,data_batch2, length_batch1,length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "       \n",
    "        #print(outputs)\n",
    "        total += label_batch.size(0)\n",
    "        correct += predicted.eq(label_batch.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/10], Step: [100/1000], Validation Acc: 44.0\n",
      "Epoch: [1/10], Step: [199/1000], Validation Acc: 46.7\n",
      "Epoch: [1/10], Step: [298/1000], Validation Acc: 43.9\n",
      "Epoch: [1/10], Step: [397/1000], Validation Acc: 47.1\n",
      "Epoch: [1/10], Step: [496/1000], Validation Acc: 54.7\n",
      "Epoch: [1/10], Step: [595/1000], Validation Acc: 51.9\n",
      "Epoch: [1/10], Step: [694/1000], Validation Acc: 55.8\n",
      "Epoch: [1/10], Step: [793/1000], Validation Acc: 58.0\n",
      "Epoch: [1/10], Step: [892/1000], Validation Acc: 60.7\n",
      "Epoch: [1/10], Step: [991/1000], Validation Acc: 61.6\n",
      "Epoch: [2/10], Step: [100/1000], Validation Acc: 60.3\n",
      "Epoch: [2/10], Step: [199/1000], Validation Acc: 62.1\n",
      "Epoch: [2/10], Step: [298/1000], Validation Acc: 62.5\n",
      "Epoch: [2/10], Step: [397/1000], Validation Acc: 62.3\n",
      "Epoch: [2/10], Step: [496/1000], Validation Acc: 63.2\n",
      "Epoch: [2/10], Step: [595/1000], Validation Acc: 62.4\n",
      "Epoch: [2/10], Step: [694/1000], Validation Acc: 65.9\n",
      "Epoch: [2/10], Step: [793/1000], Validation Acc: 64.6\n",
      "Epoch: [2/10], Step: [892/1000], Validation Acc: 64.1\n",
      "Epoch: [2/10], Step: [991/1000], Validation Acc: 66.0\n",
      "Epoch: [3/10], Step: [100/1000], Validation Acc: 65.0\n",
      "Epoch: [3/10], Step: [199/1000], Validation Acc: 64.2\n",
      "Epoch: [3/10], Step: [298/1000], Validation Acc: 66.1\n",
      "Epoch: [3/10], Step: [397/1000], Validation Acc: 65.6\n",
      "Epoch: [3/10], Step: [496/1000], Validation Acc: 65.3\n",
      "Epoch: [3/10], Step: [595/1000], Validation Acc: 64.7\n",
      "Epoch: [3/10], Step: [694/1000], Validation Acc: 65.9\n",
      "Epoch: [3/10], Step: [793/1000], Validation Acc: 66.6\n",
      "Epoch: [3/10], Step: [892/1000], Validation Acc: 66.9\n",
      "Epoch: [3/10], Step: [991/1000], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [100/1000], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [199/1000], Validation Acc: 67.1\n",
      "Epoch: [4/10], Step: [298/1000], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [397/1000], Validation Acc: 66.4\n",
      "Epoch: [4/10], Step: [496/1000], Validation Acc: 66.7\n",
      "Epoch: [4/10], Step: [595/1000], Validation Acc: 67.4\n",
      "Epoch: [4/10], Step: [694/1000], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [793/1000], Validation Acc: 67.7\n",
      "Epoch: [4/10], Step: [892/1000], Validation Acc: 68.4\n",
      "Epoch: [4/10], Step: [991/1000], Validation Acc: 68.0\n",
      "Epoch: [5/10], Step: [100/1000], Validation Acc: 69.0\n",
      "Epoch: [5/10], Step: [199/1000], Validation Acc: 66.9\n",
      "Epoch: [5/10], Step: [298/1000], Validation Acc: 70.1\n",
      "Epoch: [5/10], Step: [397/1000], Validation Acc: 67.1\n",
      "Epoch: [5/10], Step: [496/1000], Validation Acc: 66.7\n",
      "Epoch: [5/10], Step: [595/1000], Validation Acc: 68.2\n",
      "Epoch: [5/10], Step: [694/1000], Validation Acc: 68.0\n",
      "Epoch: [5/10], Step: [793/1000], Validation Acc: 66.8\n",
      "Epoch: [5/10], Step: [892/1000], Validation Acc: 68.1\n",
      "Epoch: [5/10], Step: [991/1000], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [100/1000], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [199/1000], Validation Acc: 67.3\n",
      "Epoch: [6/10], Step: [298/1000], Validation Acc: 67.1\n",
      "Epoch: [6/10], Step: [397/1000], Validation Acc: 67.9\n",
      "Epoch: [6/10], Step: [496/1000], Validation Acc: 69.5\n",
      "Epoch: [6/10], Step: [595/1000], Validation Acc: 66.1\n",
      "Epoch: [6/10], Step: [694/1000], Validation Acc: 66.6\n",
      "Epoch: [6/10], Step: [793/1000], Validation Acc: 69.7\n",
      "Epoch: [6/10], Step: [892/1000], Validation Acc: 66.9\n",
      "Epoch: [6/10], Step: [991/1000], Validation Acc: 67.8\n",
      "Epoch: [7/10], Step: [100/1000], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [199/1000], Validation Acc: 68.6\n",
      "Epoch: [7/10], Step: [298/1000], Validation Acc: 67.5\n",
      "Epoch: [7/10], Step: [397/1000], Validation Acc: 69.4\n",
      "Epoch: [7/10], Step: [496/1000], Validation Acc: 68.2\n",
      "Epoch: [7/10], Step: [595/1000], Validation Acc: 68.5\n",
      "Epoch: [7/10], Step: [694/1000], Validation Acc: 67.7\n",
      "Epoch: [7/10], Step: [793/1000], Validation Acc: 68.0\n",
      "Epoch: [7/10], Step: [892/1000], Validation Acc: 68.7\n",
      "Epoch: [7/10], Step: [991/1000], Validation Acc: 66.5\n",
      "Epoch: [8/10], Step: [100/1000], Validation Acc: 69.6\n",
      "Epoch: [8/10], Step: [199/1000], Validation Acc: 69.6\n",
      "Epoch: [8/10], Step: [298/1000], Validation Acc: 68.4\n",
      "Epoch: [8/10], Step: [397/1000], Validation Acc: 68.5\n",
      "Epoch: [8/10], Step: [496/1000], Validation Acc: 67.0\n",
      "Epoch: [8/10], Step: [595/1000], Validation Acc: 69.0\n",
      "Epoch: [8/10], Step: [694/1000], Validation Acc: 67.7\n",
      "Epoch: [8/10], Step: [793/1000], Validation Acc: 65.3\n",
      "Epoch: [8/10], Step: [892/1000], Validation Acc: 68.1\n",
      "Epoch: [8/10], Step: [991/1000], Validation Acc: 68.0\n",
      "Epoch: [9/10], Step: [100/1000], Validation Acc: 67.3\n",
      "Epoch: [9/10], Step: [199/1000], Validation Acc: 68.8\n",
      "Epoch: [9/10], Step: [298/1000], Validation Acc: 69.7\n",
      "Epoch: [9/10], Step: [397/1000], Validation Acc: 66.7\n",
      "Epoch: [9/10], Step: [496/1000], Validation Acc: 67.0\n",
      "Epoch: [9/10], Step: [595/1000], Validation Acc: 69.2\n",
      "Epoch: [9/10], Step: [694/1000], Validation Acc: 68.2\n",
      "Epoch: [9/10], Step: [793/1000], Validation Acc: 68.6\n",
      "Epoch: [9/10], Step: [892/1000], Validation Acc: 68.7\n",
      "Epoch: [9/10], Step: [991/1000], Validation Acc: 67.4\n",
      "Epoch: [10/10], Step: [100/1000], Validation Acc: 69.4\n",
      "Epoch: [10/10], Step: [199/1000], Validation Acc: 67.7\n",
      "Epoch: [10/10], Step: [298/1000], Validation Acc: 67.1\n",
      "Epoch: [10/10], Step: [397/1000], Validation Acc: 68.2\n",
      "Epoch: [10/10], Step: [496/1000], Validation Acc: 69.2\n",
      "Epoch: [10/10], Step: [595/1000], Validation Acc: 67.6\n",
      "Epoch: [10/10], Step: [694/1000], Validation Acc: 67.2\n",
      "Epoch: [10/10], Step: [793/1000], Validation Acc: 69.4\n",
      "Epoch: [10/10], Step: [892/1000], Validation Acc: 68.1\n",
      "Epoch: [10/10], Step: [991/1000], Validation Acc: 68.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "learning_rate = 0.005\n",
    "num_epochs = 10 # number epoch to train\n",
    "\n",
    "\n",
    "model = SNLI_CNN(loaded_embeddings_ft,emb_size=300, hidden_size=200, num_layers=2, num_classes=3,vocab_size=50000).cuda()\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data_1,data_2, lengths_1,lengths_2, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch1,data_batch2,length_batch1,length_batch2,label_batch = data_1.cuda(),data_2.cuda(), lengths_1.cuda(),lengths_2.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch1,data_batch2, length_batch1,length_batch2)\n",
    "\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 32 iterations\n",
    "        if i > 0 and i % (BATCH_SIZE-1) == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                        epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputErrors(loader, model,number=3):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    correct_sen = []\n",
    "    wrong_sen = []\n",
    "    \n",
    "    actual_pre_cor = []\n",
    "    actual_pre_wor = []\n",
    "    flag=0\n",
    "    \n",
    "    for data_1,data_2, lengths_1,lengths_2, labels in loader:\n",
    "        data_batch1,data_batch2, length_batch1,length_batch2, label_batch = data_1.cuda(),data_2.cuda(), lengths_1.cuda(),lengths_2.cuda(), labels.cuda()\n",
    "        outputs = F.softmax(model(data_batch1,data_batch2, length_batch1,length_batch2), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        for i in range(len(predicted)):\n",
    "            if len(correct_sen) >=number and len(wrong_sen) >=number:\n",
    "                flag=1\n",
    "                break\n",
    "            else:\n",
    "                if predicted[i]==label_batch[i]:\n",
    "                    #print(data_batch1[i],data_batch2[i])\n",
    "                    correct_sen.append((data_batch1[i],data_batch2[i]))\n",
    "                    actual_pre_cor.append((predicted[i],label_batch[i]))\n",
    "                else:\n",
    "                    wrong_sen.append((data_batch1[i],data_batch2[i]))\n",
    "                    actual_pre_wor.append((predicted[i],label_batch[i]))\n",
    "\n",
    "        if flag==1:\n",
    "            break                    \n",
    "    return actual_pre_cor,actual_pre_wor,correct_sen,wrong_sen\n",
    "\n",
    "actual_pre_cor,actual_pre_wor,cor,wrong = outputErrors(val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Print the Corrected predicitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is the prediction while the second one is the actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([1], device='cuda:0'), tensor(1, device='cuda:0'))\n",
      "the Premise\n",
      "A man wearing a suit with a name tag stands reading in front of a microphone as a screen behind him displays a presentation .\n",
      "the Hypothesis\n",
      "A man with a microphone is standing in front of a screen .\n",
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([1], device='cuda:0'), tensor(1, device='cuda:0'))\n",
      "the Premise\n",
      "The room full of youths reacts emotionally as they <unk> .\n",
      "the Hypothesis\n",
      "people <unk> emotional watching something .\n",
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([2], device='cuda:0'), tensor(2, device='cuda:0'))\n",
      "the Premise\n",
      "A man selling donuts to a customer during a world exhibition event held in the city of Angeles\n",
      "the Hypothesis\n",
      "A woman drinks her coffee in a small cafe .\n",
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([2], device='cuda:0'), tensor(2, device='cuda:0'))\n",
      "the Premise\n",
      "The man and woman are both smiling .\n",
      "the Hypothesis\n",
      "A man and a woman are <unk> looking at the clock .\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for c in cor:\n",
    "    print('\\n')\n",
    "    print(\"0 for neutral; 1 for entailment; 2 for contradiction\")\n",
    "\n",
    "    print (actual_pre_cor[count])\n",
    "    print('the Premise')\n",
    "    print((\" \").join([ordered_words_ft[i] for i in np.array(c[0]) if i !=0]))\n",
    "    print('the Hypothesis')\n",
    "    print((\" \").join([ordered_words_ft[i] for i in np.array(c[1]) if i !=0]))\n",
    "\n",
    "    count+=1\n",
    "    if count>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Print the Wrong predicitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is the prediction while the second one is the actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([0], device='cuda:0'), tensor(1, device='cuda:0'))\n",
      "the Premise\n",
      "Two male curling players are on ice sweeping the path in front of polished rock , a small crowd watches .\n",
      "the Hypothesis\n",
      "Several male curling players sweeping their path on ice in front of a rock as a small crowd watches .\n",
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([1], device='cuda:0'), tensor(0, device='cuda:0'))\n",
      "the Premise\n",
      "A black girl and white girl walking hand and hand in a busy area of a city with a public train in the background .\n",
      "the Hypothesis\n",
      "Some kids take public transit .\n",
      "\n",
      "\n",
      "0 for neutral; 1 for entailment; 2 for contradiction\n",
      "(tensor([1], device='cuda:0'), tensor(2, device='cuda:0'))\n",
      "the Premise\n",
      "These two people in hats are standing on rocky terrain , with their arms around each other .\n",
      "the Hypothesis\n",
      "Two people on rocky terrain waving at each other with both arms .\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for w in wrong:\n",
    "    print('\\n')\n",
    "    print(\"0 for neutral; 1 for entailment; 2 for contradiction\")\n",
    "    print (actual_pre_wor[count])\n",
    "    \n",
    "    print('the Premise')\n",
    "    print((\" \").join([ordered_words_ft[i] for i in np.array(w[0]) if i !=0]))\n",
    "    print('the Hypothesis')\n",
    "    print((\" \").join([ordered_words_ft[i] for i in np.array(w[1]) if i !=0]))\n",
    "\n",
    "    count+=1\n",
    "    if count>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Output the accuracy results on MultiNLI dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN\n",
      "[('fiction', 43.91959798994975), ('telephone', 44.875621890547265), ('slate', 40.31936127744511), ('government', 46.8503937007874), ('travel', 43.58452138492871)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "res=[]\n",
    "for g in test_data['genre'].unique():\n",
    "    test_data_sentence1_indices = token2index_dataset(test_data[test_data['genre']==g]['sentence1'])\n",
    "    test_data_sentence2_indices = token2index_dataset(test_data[test_data['genre']==g]['sentence2'])\n",
    "    test_targets_g = test_targets[test_data['genre']==g]\n",
    "    \n",
    "    test_dataset = NewsGroupDataset(test_data_sentence1_indices,test_data_sentence2_indices, test_targets_g)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=newsgroup_collate_func,\n",
    "                                               shuffle=True)\n",
    "    res.append((g, test_model(test_loader, model)))\n",
    "    \n",
    "\n",
    "print(\"CNN\")\n",
    "print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
